{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+oSBYRX2doF9lyfgNWeta",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Divyanshi-16/Information-Retrieval-3/blob/main/Marathi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Name: Divyanshi Chauhan\n",
        "*   Roll No.: 21074012\n",
        "\n",
        "*   Discipline: Computer Science and Engineering(IDD)\n",
        "*   Use source.zip file to upload in the files section in google colab"
      ],
      "metadata": {
        "id": "RDfv-SOu77Wj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbgSbEgM6V30",
        "outputId": "4e916d84-ac1a-45e6-8f7d-b88664030d10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "file_name = \"/content/source.zip\"\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "    zip.extractall()\n",
        "    print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_file_path = '/content/stopwords-mr.txt'\n",
        "\n",
        "with open(stopwords_file_path, 'r', encoding='utf-8') as file:\n",
        "    stopwords_marathi = set(file.read().split())"
      ],
      "metadata": {
        "id": "ncQHgCpu63FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def load_marathi_stopwords(stopwords_file_path):\n",
        "    with open(stopwords_file_path, 'r', encoding='utf-8') as stopwords_file:\n",
        "        marathi_stopwords = stopwords_file.read().splitlines()\n",
        "    return set(marathi_stopwords)\n",
        "\n",
        "def perform_marathi_stemming(word):\n",
        "    suffixes = {\n",
        "        1: [\"ो\", \"े\", \"ू\", \"ु\", \"ी\", \"ि\", \"ा\", \"च\"],\n",
        "        2: [\"चा\", \"चे\", \"ने\", \"नी\", \"ना\", \"ते\", \"ीं\", \"तील\", \"ात\", \"ाँ\", \"ां\", \"ों\", \"ें\", \"तच\", \"ता\", \"ही\", \"ले\"],\n",
        "        3: [\"ाचा\", \"ाचे\", \"तील\", \"ानी\", \"ाने\", \"ाना\", \"ाते\", \"ाती\", \"ाता\", \"तीं\", \"तून\", \"तील\", \"तही\", \"तपण\", \"कडे\", \"ातच\",\n",
        "            \"हून\", \"पणे\", \"ाही\", \"ाले\"],\n",
        "        4: [\"मधले\", \"ातील\", \"च्या\", \"न्या\", \"ऱ्या\", \"ख्या\", \"वर\", \"साठी\", \"ातून\", \"कडून\", \"मुळे\", \"वरून\", \"ातील\", \"नीही\",\n",
        "            \"ातही\", \"ातपण\", \"ाकडे\", \"पाशी\", \"ाहून\", \"ापणे\", \"मधला\"],\n",
        "        5: [\"ामधले\", \"ाच्या\", \"ान्या\", \"ाऱ्या\", \"ाख्या\", \"ावर\", \"ासाठी\", \"पासून\", \"ाकडून\", \"ामुळे\", \"ावरून\", \"कडेही\",\n",
        "            \"ानीही\", \"ापाशी\", \"ामधला\", \"मध्ये\"],\n",
        "        6: [\"पर्यंत\", \"ापासून\", \"ाकडेही\", \"पूर्वक\", \"लेल्या\", \"ामध्ये\"],\n",
        "        7: [\"ापर्यंत\", \"प्रमाणे\", \"तसुद्धा\", \"ापूर्वक\", \"ालेल्या\"],\n",
        "        8: [\"ाप्रमाणे\", \"ातसुद्धा\"],\n",
        "    }\n",
        "\n",
        "    for i in range(8, 0, -1):\n",
        "        if len(word) > i + 1:\n",
        "            for suf in suffixes[i]:\n",
        "                if word.endswith(suf):\n",
        "                    return word[:-i]\n",
        "    return word\n",
        "\n",
        "def process_marathi_text(text, marathi_stopwords):\n",
        "    # Remove English alphabet characters, digits, and special characters using regex\n",
        "    text = re.sub(r'[^ ऀ-ॿ]+', ' ', text)\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(marathi_stopwords)\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Perform stemming/lemmatization on Marathi words\n",
        "    marathi_words = [perform_marathi_stemming(word) for word in words]\n",
        "\n",
        "    return marathi_words\n",
        "\n",
        "def create_index(folder_path, marathi_stopwords):\n",
        "    term_frequency = defaultdict(lambda: defaultdict(int))\n",
        "    document_frequency = defaultdict(int)\n",
        "    posting_list = defaultdict(list)\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "\n",
        "            marathi_words = process_marathi_text(text, marathi_stopwords)\n",
        "\n",
        "            for term in set(marathi_words):\n",
        "                term_frequency[term][filename] += marathi_words.count(term)\n",
        "                document_frequency[term] += 1\n",
        "\n",
        "            for term in set(marathi_words):\n",
        "                posting_list[term].append((filename, marathi_words.count(term)))\n",
        "\n",
        "    return term_frequency, document_frequency, posting_list\n",
        "\n",
        "def vectorize_query(query, marathi_stopwords):\n",
        "    query_terms = process_marathi_text(query, marathi_stopwords)\n",
        "    query_vector = defaultdict(float)\n",
        "    for term in query_terms:\n",
        "        query_vector[term] += 1\n",
        "    return query_vector\n",
        "\n",
        "def compute_tf_idf(term_frequency, document_frequency, num_docs, doc_lengths):\n",
        "    tfidf_index = defaultdict(dict)\n",
        "    for term, doc_tf in term_frequency.items():\n",
        "        idf = math.log(num_docs / document_frequency[term])\n",
        "        for doc, tf in doc_tf.items():\n",
        "            tfidf_index[term][doc] = (1 + math.log(tf)) * idf / doc_lengths[doc]\n",
        "    return tfidf_index\n",
        "\n",
        "def compute_doc_lengths(posting_list, num_docs):\n",
        "    doc_lengths = defaultdict(float)\n",
        "    for term, postings in posting_list.items():\n",
        "        idf = math.log(num_docs / len(postings))\n",
        "        for doc, tf in postings:\n",
        "            doc_lengths[doc] += (1 + math.log(tf)) * idf\n",
        "    return {doc: math.sqrt(length) for doc, length in doc_lengths.items()}\n",
        "\n",
        "def cosine_similarity(query_vector, tfidf_index, doc_lengths):\n",
        "    scores = defaultdict(float)\n",
        "    for term, query_weight in query_vector.items():\n",
        "        if term in tfidf_index:\n",
        "            for doc, doc_weight in tfidf_index[term].items():\n",
        "                scores[doc] += query_weight * doc_weight\n",
        "    for doc, score in scores.items():\n",
        "        scores[doc] /= doc_lengths[doc]\n",
        "    return scores\n",
        "\n",
        "folder_path = '/content/source'\n",
        "\n",
        "stopwords_file_path = '/content/stopwords-mr.txt'\n",
        "marathi_stopwords = load_marathi_stopwords(stopwords_file_path)\n",
        "\n",
        "term_frequency, document_frequency, posting_list = create_index(folder_path, marathi_stopwords)\n",
        "num_docs = len(os.listdir(folder_path))\n",
        "doc_lengths = compute_doc_lengths(posting_list, num_docs)\n",
        "\n",
        "tfidf_index = compute_tf_idf(term_frequency, document_frequency, num_docs, doc_lengths)\n",
        "\n",
        "query = \"आपलं समन्वय\"\n",
        "query_vector = vectorize_query(query, marathi_stopwords)\n",
        "scores = cosine_similarity(query_vector, tfidf_index, doc_lengths)\n",
        "\n",
        "# Print top 5 relevant documents\n",
        "print(\"Query:\", query)\n",
        "ranked_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "print(\"Relevant documents:\")\n",
        "for doc, score in ranked_docs:\n",
        "    print(f\"Document: {doc}, Score: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA7_xD0R7aXY",
        "outputId": "47bc268b-c837-4630-c80f-6da4521d3c46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: आपलं समन्वय\n",
            "Relevant documents:\n",
            "Document: Kolhapur2414285894.htm.txt, Score: 0.0229\n",
            "Document: GoaD5AE4D8881.htm.txt, Score: 0.0196\n",
            "Document: Mumbai8E244E3F6E.htm.txt, Score: 0.0189\n",
            "Document: Maharashtra24998CBA97.htm.txt, Score: 0.0166\n",
            "Document: National91D06E120F.htm.txt, Score: 0.0152\n"
          ]
        }
      ]
    }
  ]
}